{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "import io\n",
    "import glob\n",
    "\n",
    "from src.fetch_data_pipeline import extract_image_url, extract_df, download_images, load_RG_data, zip_lookup, gps_lookup\n",
    "import json\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16.VGG16(include_top = True, weights = 'imagenet')\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.outputs = [model.layers[-1].output]\n",
    "#remove the next fully connected layer (fc7)\n",
    "#remove the classification layer (fc8) softmax?\n",
    "#Stripped down model with 4096 category output\n",
    "#fix the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full vgg16\n",
    "[<keras.engine.input_layer.InputLayer at 0x1aafbcf2b0>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1ab00d4e80>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1ab00d46d8>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0x1ab0bd20f0>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1aae8eedd8>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1aaf356dd8>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0x1aaf84ac88>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1a7025d550>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1aae42fa90>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1aae8867b8>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0x1aaddc9a58>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1aac093940>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1aaba4e2b0>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1aaa1bccc0>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0x1a6e7e6940>,\n",
    " <keras.layers.convolutional.Conv2D at 0x1a6dafbda0>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb2f0ea0b8>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb2f0ea2b0>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0xb24c75c18>,\n",
    " <keras.layers.core.Flatten at 0xb18b03f28>,\n",
    " <keras.layers.core.Dense at 0xb2f333518>,\n",
    " <keras.layers.core.Dense at 0x1aaa750dd8>,\n",
    " <keras.layers.core.Dense at 0x1ab3fa36d8>]\n",
    "\n",
    "#Original pop off\n",
    "<keras.engine.input_layer.InputLayer at 0xb24363a90>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb24350668>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb17f81da0>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0xb17f81c88>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb18574080>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb18574da0>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0xb18592550>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb185ab898>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb185c7cf8>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb185e23c8>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0xb185fca20>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb18619b70>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb18633ef0>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb1864e3c8>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0xb18665780>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb189c38d0>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb189e0c50>,\n",
    " <keras.layers.convolutional.Conv2D at 0xb189e0f60>,\n",
    " <keras.layers.pooling.MaxPooling2D at 0xb18a0d6d8>,\n",
    " <keras.layers.core.Flatten at 0xb18a2d630>,\n",
    " <keras.layers.core.Dense at 0xb18a2dd30>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename_a = 'data/aww.png'\n",
    "#filename_a = 'data/test1.png'\n",
    "#filename_a = 'data/test2.png'\n",
    "#filename_a = 'data/test3.png'\n",
    "#filename_a = 'data/mossy1.png'\n",
    "#filename_a = 'data/Nibu.png'\n",
    "filename_a = 'data/dyno.png'\n",
    "#filename_a = 'data/cool.png'\n",
    "\n",
    "# Load image\n",
    "original_a = load_img(filename_a, target_size=(224, 224))\n",
    "print('PIL image size', original_a.size)\n",
    "plt.imshow(original_a)\n",
    "plt.show()\n",
    " \n",
    "# Convert image to numpy array\n",
    "# IN PIL - image is in (width, height, channel)\n",
    "# In Numpy - image is in (height, width, channel)\n",
    "numpy_image_a = img_to_array(original_a)\n",
    "plt.imshow(np.uint8(numpy_image_a))\n",
    "plt.show()\n",
    "print('numpy array size',numpy_image_a.shape)\n",
    " \n",
    "# Convert the image into batch format\n",
    "# expand_dims will add an extra dimension to the data at a particular axis\n",
    "# We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n",
    "# Thus we add the extra dimension to the axis 0.\n",
    "image_batch_a = np.expand_dims(numpy_image_a, axis=0)\n",
    "print('image batch size', image_batch_a.shape)\n",
    "plt.imshow(np.uint8(image_batch_a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the image for the VGG model\n",
    "processed_image_a = vgg16.preprocess_input(image_batch_a.copy())\n",
    "\n",
    "# get the predicted probabilities for each class\n",
    "predictions_a = model.predict(processed_image_a)\n",
    "\n",
    "print(predictions_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array_4750 = np.load('web/static/temp/data/doggie_features_4750.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_url = combined_df.ImageUrl[0:4750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_matches(labels,score):\n",
    "    sorted_scores = sorted(list(zip(labels.tolist(),score)), key = lambda t: t[1])\n",
    "    return sorted_scores[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df, combined_imgs = load_RG_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image file name, store in Pandas Series and pickle for website\n",
    "image_names = []\n",
    "for image_url in combined_imgs.ImageUrl[0:140741]:\n",
    "        image_names.append(image_url.split('/')[-1])\n",
    "\n",
    "fetch_image_names = pd.Series(image_names)\n",
    "fetch_image_names.to_pickle('data/fetch_img_urls.pkl', compression='gzip')\n",
    "\n",
    "\n",
    "\n",
    "#combined_df.head()\n",
    "#zipped_dogs = list(zip(dog_url.tolist(),results))\n",
    "#sorted_zipped_dogs = sorted(zipped_dogs, key = lambda t: t[1])\n",
    "#top_10 = sorted_zipped_dogs[0:11]\n",
    "#top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durka_list = pd.read_pickle('data/fetch_img_urls.pkl', compression='gzip')\n",
    "durka_list.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 - Random\n",
    "\n",
    "for x in range(0,11):\n",
    "    image = random.choice(dog_url.tolist())\n",
    "    plt.imshow(load_img('/Users/bil2ab/galvanize/RG5kimages/'+image.split('/')[-1]))\n",
    "    plt.show()\n",
    "    #print(1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load NumPy data file of 4750 dogs and calculate average number of non-zero features in 1D arrays for all dogs\n",
    "\n",
    "non_zero_features = []\n",
    "feature_array_4750 = np.load('web/static/temp/data/doggie_features_4750.npy')\n",
    "\n",
    "for dog in feature_array_4750:\n",
    "    non_zero_features.append(len(np.where(dog>0)[1]))\n",
    "\n",
    "plt.hist(non_zero_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scores in score_lists:\n",
    "    for image,score in top_ten(dog_url, scores):\n",
    "        plt.imshow(load_img('/Users/bil2ab/galvanize/RG5kimages/'+image.split('/')[-1]))\n",
    "        plt.show()\n",
    "        #print(1-score)\n",
    "        #print('DURKA DURKA...next distance metric:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    start = time.time()\n",
    "    feature_array_list = []\n",
    "    #image_path_list formerly combined_df.ImageUrl[0:4750]\n",
    "    for url in image_path_list[0:length]:\n",
    "        image_path = '/Users/bil2ab/galvanize/capstone/images_to_vectorize/images/'+url.split('/')[-1]\n",
    "        dog = load_img(image_path, target_size=(224, 224))\n",
    "        numpy_image = img_to_array(dog)\n",
    "        image_batch = np.expand_dims(numpy_image, axis=0)  \n",
    "        processed_image = vgg16.preprocess_input(image_batch.copy())\n",
    "        feature_array = model.predict(processed_image)\n",
    "        feature_array_list.append(feature_array)\n",
    "        #doggie = np.asarray(feature_array_list)\n",
    "        #np.save('data/RG_50k_features', doggie)\n",
    "    end = time.time()\n",
    "    total_time = end-start\n",
    "    print('Total Time: '+str(total_time))\n",
    "    print('All dog features vectorized!')\n",
    "    return feature_array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_vec = combined_imgs.ImageUrl.tolist()\n",
    "holy_durka = vectorize_dog_images(url_vec, length=50000)\n",
    "#len(url_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def durka():\n",
    "    '''\n",
    "    image_path_list = combined_df.ImageUrl.tolist()\n",
    "    Take collection of dog images and vectorize each image to a 1D NumPy array. \n",
    "    INPUT: List, Pandas Series, some iterable of filepaths to dog images (strings)\n",
    "    OUTPUT: Returns Numpy data file\n",
    "    '''\n",
    "    start = time.time()\n",
    "    combined_df, combined_imgs = load_RG_data()\n",
    "    #num_images = len(glob.glob1('/Users/bil2ab/galvanize/RG5kimages/','*.jpg'))\n",
    "    image_path_list = combined_imgs.ImageUrl.tolist()\n",
    "    feature_matrix = np.zeros((len(image_path_list),4096))\n",
    "    \n",
    "    for idx,url in enumerate(image_path_list):\n",
    "        dog = load_img('/Users/bil2ab/galvanize/RG5kimages/'+url.split('/')[-1], target_size=(224, 224))\n",
    "        image_batch = np.expand_dims(img_to_array(dog), axis=0)  \n",
    "        processed_image = vgg16.preprocess_input(image_batch.copy())\n",
    "        feature_matrix[idx] = model.predict(processed_image)\n",
    "    \n",
    "    #Save csv of image urls\n",
    "    image_path_list.to_csv('/data/dog_urls_test.csv')\n",
    "    \n",
    "    #Save list of feature arrays as numpy data file\n",
    "    #doggie = np.asarray(feature_array_list)\n",
    "    np.save('/data/doggie_features_test', feature_matrix)\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Total Time: '+str(end-start))\n",
    "    print('All dog features vectorized!')\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.dot(predictions_a, feature_matrix.T) / (np.linalg.norm(feature_matrix, axis=1) * np.linalg.norm(predictions_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit distance.cdist(predictions_a,feature_matrix,'cosine')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = combined_df.ImageUrl.tolist()[0:1000]\n",
    "score = distance.cdist(predictions_a,feature_matrix,'cosine')[0].tolist()\n",
    "\n",
    "if len(labels) == len(score):\n",
    "    sorted_scores = sorted(list(zip(labels,score)), key = lambda t: t[1])\n",
    "else:\n",
    "    print('Length mismatch!')\n",
    "\n",
    "for image, score in sorted_scores[0:10]:\n",
    "    plt.imshow(load_img('/Users/bil2ab/galvanize/RG5kimages/'+image.split('/')[-1]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_activation\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "# Utility to search for layer index by name.\n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'preds')\n",
    "\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "# This is the output node we want to maximize.\n",
    "filter_idx = 0\n",
    "img = visualize_activation(model, layer_idx, filter_indices=filter_idx)\n",
    "plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the VGG model\n",
    "#vgg_model = vgg16.VGG16(weights='imagenet')\n",
    "\n",
    "#Load the Inception_V3 model\n",
    "#inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    " \n",
    "#Load the ResNet50 model\n",
    "#resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    " \n",
    "#Load the MobileNet model\n",
    "#mobilenet_model = mobilenet.MobileNet(weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract exif data from smartphone image and view in nice format\n",
    "\n",
    "from PIL.ExifTags import TAGS\n",
    "\n",
    "#def extract_image_data(file):\n",
    "filename =''\n",
    "im = PIL.Image.open(filename)\n",
    "exifdict = im._getexif()\n",
    "#print(exifdict)\n",
    "\n",
    "if len(exifdict):\n",
    "    for k in exifdict.keys():\n",
    "        if k in TAGS.keys():\n",
    "            print(TAGS[k], exifdict[k])\n",
    "        else:\n",
    "            print(k, exifdict[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
